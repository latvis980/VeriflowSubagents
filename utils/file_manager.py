import os
from pathlib import Path
from datetime import datetime
import asyncio

# Import the separate publication name extractor
from utils.publication_name_extractor import get_publication_name_extractor


class FileManager:
    """Manage temporary storage of scraped content"""

    def __init__(self, temp_dir: str = "temp"):
        self.temp_dir = Path(temp_dir)
        self.temp_dir.mkdir(exist_ok=True)

        # Track page titles for AI name extraction
        self.page_titles = {}

    def create_session(self) -> str:
        """Create unique session directory"""
        session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        session_path = self.temp_dir / session_id
        session_path.mkdir(exist_ok=True)
        return session_id

    def set_page_title(self, url: str, title: str):
        """
        Store page title for a URL (optional, improves AI extraction quality)

        Args:
            url: The source URL
            title: The page title from <title> tag
        """
        self.page_titles[url] = title

    def save_session_content(
        self, 
        session_id: str, 
        all_scraped_content: dict, 
        facts: list = None,
        upload_to_drive: bool = False,
        queries_by_fact: dict = None
    ):
        """
        Save all scraped content with metadata in one comprehensive file

        Args:
            session_id: Unique session identifier
            all_scraped_content: Dict of scraped content
            facts: List of facts being verified
            upload_to_drive: If True, upload the report to Google Drive after saving
            queries_by_fact: Dict mapping fact_id to SearchQueries object (optional)
        """
        session_path = self.temp_dir / session_id
        filepath = session_path / "session_report.txt"

        # Extract publication names using AI
        publication_names = asyncio.run(self._extract_all_publication_names(all_scraped_content.keys()))

        with open(filepath, 'w', encoding='utf-8') as f:
            # Header with session metadata
            f.write("=" * 100 + "\n")
            f.write("FACT-CHECK SESSION REPORT\n")
            f.write(f"Session ID: {session_id}\n")
            f.write(f"Generated: {datetime.now().isoformat()}\n")
            f.write(f"Total Sources: {len(all_scraped_content)}\n")
            if facts:
                f.write(f"Total Facts Analyzed: {len(facts)}\n")
            f.write("=" * 100 + "\n\n")

            # Table of Contents
            f.write("TABLE OF CONTENTS:\n")
            f.write("-" * 50 + "\n")
            for i, url in enumerate(all_scraped_content.keys(), 1):
                publication_name = publication_names.get(url, "Unknown Source")
                f.write(f"{i:2d}. {publication_name}\n")
                f.write(f"    URL: {url}\n")
            f.write("\n" + "=" * 100 + "\n\n")

            # Facts being checked (if provided)
            if facts:
                f.write("FACTS BEING VERIFIED:\n")
                f.write("-" * 50 + "\n")
                for fact in facts:
                    f.write(f"â€¢ {fact.id}: {fact.statement}\n")
                f.write("\n" + "=" * 100 + "\n\n")

            # Generated Search Queries Section (if provided)
            if queries_by_fact:
                f.write("GENERATED SEARCH QUERIES:\n")
                f.write("=" * 100 + "\n")
                f.write("This section shows all search queries generated by the Query Generator\n")
                f.write("for each fact. These queries were used to find sources via web search.\n")
                f.write("=" * 100 + "\n\n")

                for fact_id, queries in queries_by_fact.items():
                    fact_statement = queries.fact_statement if hasattr(queries, 'fact_statement') else "N/A"

                    f.write(f"FACT ID: {fact_id}\n")
                    f.write("-" * 80 + "\n")
                    f.write(f"Statement: {fact_statement}\n\n")

                    f.write("PRIMARY QUERY:\n")
                    f.write(f"  â†’ {queries.primary_query}\n\n")

                    if queries.alternative_queries:
                        f.write(f"ALTERNATIVE QUERIES ({len(queries.alternative_queries)}):\n")
                        for i, alt_query in enumerate(queries.alternative_queries, 1):
                            f.write(f"  {i}. {alt_query}\n")
                        f.write("\n")

                    if hasattr(queries, 'search_focus') and queries.search_focus:
                        f.write(f"SEARCH FOCUS: {queries.search_focus}\n\n")

                    if hasattr(queries, 'key_terms') and queries.key_terms:
                        f.write(f"KEY TERMS: {', '.join(queries.key_terms)}\n\n")

                    if hasattr(queries, 'expected_sources') and queries.expected_sources:
                        f.write(f"EXPECTED SOURCE TYPES: {', '.join(queries.expected_sources)}\n\n")

                    f.write("=" * 80 + "\n\n")

                f.write("\n" + "=" * 100 + "\n\n")

            # Full scraped content for each source
            for i, (url, content) in enumerate(all_scraped_content.items(), 1):
                publication_name = publication_names.get(url, "Unknown Source")
                content_length = len(content) if content else 0

                f.write(f"SOURCE #{i}: {publication_name.upper()}\n")
                f.write("=" * 80 + "\n")
                f.write(f"Publication: {publication_name}\n")
                f.write(f"URL: {url}\n")
                f.write(f"Content Length: {content_length:,} characters\n")
                f.write(f"Domain: {self._extract_domain(url)}\n")
                f.write(f"Scraped: {datetime.now().isoformat()}\n")
                f.write("-" * 80 + "\n\n")

                if content and content.strip():
                    f.write("CONTENT:\n")
                    f.write(content)
                else:
                    f.write("âŒ NO CONTENT SCRAPED (Check scraping logs for errors)\n")

                f.write("\n\n" + "=" * 100 + "\n\n")

            # Footer
            f.write("END OF SESSION REPORT\n")
            f.write("=" * 100 + "\n")

        # Upload to Google Drive if requested
        if upload_to_drive:
            try:
                from utils.gdrive_uploader import upload_session_to_drive
                from utils.logger import fact_logger

                fact_logger.logger.info(f"ðŸ“¤ Uploading session {session_id} to Google Drive")
                file_id = upload_session_to_drive(session_id, str(filepath))

                if file_id:
                    fact_logger.logger.info(
                        f"âœ… Session {session_id} uploaded to Google Drive",
                        extra={"session_id": session_id, "file_id": file_id}
                    )
                else:
                    fact_logger.logger.warning(
                        f"âš ï¸ Failed to upload session {session_id} to Google Drive"
                    )
            except ImportError:
                from utils.logger import fact_logger
                fact_logger.logger.warning(
                    "âš ï¸ Google Drive uploader not available. Install google-api-python-client"
                )
            except Exception as e:
                from utils.logger import fact_logger
                fact_logger.logger.error(
                    f"âŒ Error uploading to Google Drive: {e}",
                    extra={"session_id": session_id, "error": str(e)}
                )

    async def _extract_all_publication_names(self, urls: list) -> dict:
        """
        Extract publication names for all URLs using AI

        Args:
            urls: List of URLs to process

        Returns:
            Dict mapping URL to publication name
        """
        extractor = get_publication_name_extractor()
        results = {}

        # Process all URLs concurrently
        tasks = []
        for url in urls:
            page_title = self.page_titles.get(url)
            tasks.append((url, extractor.extract_name(url, page_title)))

        # Wait for all to complete
        for url, task in tasks:
            try:
                name = await task
                results[url] = name
            except Exception as e:
                print(f"âš ï¸ Failed to extract name for {url}: {e}")
                # Fallback to domain extraction
                results[url] = await extractor.extract_name(url, None)

        return results

    def _extract_domain(self, url: str) -> str:
        """Extract clean domain from URL"""
        from urllib.parse import urlparse
        return urlparse(url).netloc

    def _sanitize_url(self, url: str) -> str:
        """Convert URL to safe filename"""
        return url.replace('https://', '').replace('http://', '')\
                  .replace('/', '_').replace(':', '_')[:50]

    def cleanup_old_sessions(self, days: int = 1):
        """Remove sessions older than specified days"""
        # Implementation for cleanup
        pass